{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.constants import entity_unit_map\n",
    "import ast\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom dataset\n",
    "class ProductImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.entity_names = sorted(self.data['entity_name'].unique())\n",
    "        self.entity_name_to_idx = {name: idx for idx, name in enumerate(self.entity_names)}\n",
    "        self.entity_units = {entity: sorted(units) for entity, units in entity_unit_map.items()}\n",
    "        self.max_units = max(len(units) for units in self.entity_units.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.basename(self.data.iloc[idx]['image_link'])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        entity_name = self.data.iloc[idx]['entity_name']\n",
    "        entity_value, entity_unit = self.parse_entity_value(self.data.iloc[idx]['entity_value'])\n",
    "        \n",
    "        entity_name_tensor = torch.zeros(len(self.entity_names), dtype=torch.float32)\n",
    "        entity_name_tensor[self.entity_name_to_idx[entity_name]] = 1\n",
    "        \n",
    "        entity_unit_tensor = torch.zeros(self.max_units, dtype=torch.float32)\n",
    "        if entity_name in self.entity_units and entity_unit in self.entity_units[entity_name]:\n",
    "            unit_idx = self.entity_units[entity_name].index(entity_unit)\n",
    "            entity_unit_tensor[unit_idx] = 1\n",
    "        \n",
    "        return img, entity_name_tensor, torch.tensor(entity_value, dtype=torch.float32), entity_unit_tensor, entity_name\n",
    "\n",
    "    def parse_entity_value(self, value_str):\n",
    "        try:\n",
    "            value, unit = value_str.rsplit(' ', 1)\n",
    "            if unit == 'fluid':\n",
    "                value, unit = value_str.rsplit(' ', 2)[0], ' '.join(value_str.rsplit(' ', 2)[1:])\n",
    "            return float(value), unit\n",
    "        except ValueError:\n",
    "            try:\n",
    "                value_list = ast.literal_eval(value_str)\n",
    "                if isinstance(value_list, list) and len(value_list) == 2:\n",
    "                    return sum(value_list) / 2, 'unknown'\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return 0.0, 'unknown'\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_dataset = ProductImageDataset('dataset/train.csv', TRAIN_IMAGES_DIR, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Custom model\n",
    "class ProductImageModel(nn.Module):\n",
    "    def __init__(self, num_entity_names, max_units):\n",
    "        super(ProductImageModel, self).__init__()\n",
    "        self.resnet = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.fc_entity_name = nn.Linear(num_ftrs, num_entity_names).float()\n",
    "        self.fc_entity_value = nn.Linear(num_ftrs, 1).float()\n",
    "        self.fc_entity_units = nn.Linear(num_ftrs, max_units).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        entity_name_out = self.fc_entity_name(features)\n",
    "        entity_value_out = self.fc_entity_value(features).squeeze(1)\n",
    "        entity_units_out = self.fc_entity_units(features)\n",
    "        return entity_name_out, entity_value_out, entity_units_out\n",
    "\n",
    "# Initialize model\n",
    "model = ProductImageModel(len(train_dataset.entity_names), train_dataset.max_units)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_classification = nn.BCEWithLogitsLoss()\n",
    "criterion_regression = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    for images, entity_names, entity_values, entity_units, entity_name_str in progress_bar:\n",
    "        images = images.to(DEVICE).float()\n",
    "        entity_names = entity_names.to(DEVICE).float()\n",
    "        entity_values = entity_values.to(DEVICE).float()\n",
    "        entity_units = entity_units.to(DEVICE).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        entity_name_out, entity_value_out, entity_units_out = model(images)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_entity_name = criterion_classification(entity_name_out, entity_names)\n",
    "        loss_entity_value = criterion_regression(entity_value_out, entity_values)\n",
    "        loss_entity_units = criterion_classification(entity_units_out, entity_units)\n",
    "        \n",
    "        loss = loss_entity_name + loss_entity_value + loss_entity_units\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_loss,\n",
    "}, 'product_image_model.pth')\n",
    "\n",
    "print(\"Training completed. Model saved.\")\n",
    "\n",
    "# Evaluation and prediction function\n",
    "def predict(model, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        entity_name_out, entity_value_out, entity_units_out = model(image.unsqueeze(0).to(DEVICE).float())\n",
    "        \n",
    "        predicted_entity = train_dataset.entity_names[torch.argmax(entity_name_out).item()]\n",
    "        predicted_value = entity_value_out.item()\n",
    "        predicted_unit_idx = torch.argmax(entity_units_out).item()\n",
    "        predicted_unit = train_dataset.entity_units.get(predicted_entity, ['unknown'])[predicted_unit_idx] if predicted_unit_idx < len(train_dataset.entity_units.get(predicted_entity, [])) else 'unknown'\n",
    "        \n",
    "        return predicted_entity, predicted_value, predicted_unit\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, _, _, _, entity_name_str in tqdm(train_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(DEVICE).float()\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            predicted_entity, predicted_value, predicted_unit = predict(model, image)\n",
    "            formatted_prediction = f\"{predicted_value:.2f} {predicted_unit}\"\n",
    "            predictions.append(formatted_prediction)\n",
    "            \n",
    "            if predicted_entity == entity_name_str[i]:\n",
    "                correct_predictions += 1\n",
    "            total_samples += 1\n",
    "\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save predictions\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "test_df['prediction'] = predictions[:len(test_df)]  # Ensure we have the correct number of predictions\n",
    "test_df[['index', 'prediction']].to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to test_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import ast\n",
    "\n",
    "# Allow loading truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "TEST_IMAGES_DIR = 'test_images'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ACCUMULATION_STEPS = 2\n",
    "CHECKPOINT_PATH = 'product_image_model_checkpoint.pth'\n",
    "MODEL_PATH = 'product_image_model_final.pth'\n",
    "TRAIN_CSV_PATH = 'dataset/train.csv'\n",
    "TEST_CSV_PATH = 'dataset/test.csv'\n",
    "OUTPUT_CSV_PATH = 'dataset/test_out.csv'\n",
    "\n",
    "# Load entity_unit_map\n",
    "with open('src/constants.py', 'r') as f:\n",
    "    exec(f.read())\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    original_count = len(data)\n",
    "    \n",
    "    def is_valid_sample(row):\n",
    "        try:\n",
    "            value_str = row['entity_value']\n",
    "            entity_name = row['entity_name']\n",
    "            value, unit = value_str.rsplit(' ', 1)\n",
    "            if unit == 'fluid':\n",
    "                value, unit = value_str.rsplit(' ', 2)[0], ' '.join(value_str.rsplit(' ', 2)[1:])\n",
    "            float(value)  # Check if value can be converted to float\n",
    "            return unit in entity_unit_map.get(entity_name, [])\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    filtered_data = data[data.apply(is_valid_sample, axis=1)]\n",
    "    filtered_count = len(filtered_data)\n",
    "    \n",
    "    print(f\"Original sample count: {original_count}\")\n",
    "    print(f\"Filtered sample count: {filtered_count}\")\n",
    "    print(f\"Removed {original_count - filtered_count} samples ({(original_count - filtered_count) / original_count:.2%})\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Custom dataset\n",
    "class ProductImageDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, transform=None):\n",
    "        self.data = data\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.entity_names = sorted(self.data['entity_name'].unique())\n",
    "        self.entity_name_to_idx = {name: idx for idx, name in enumerate(self.entity_names)}\n",
    "        self.entity_units = {entity: sorted(units) for entity, units in entity_unit_map.items()}\n",
    "        self.max_units = max(len(units) for units in self.entity_units.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.basename(self.data.iloc[idx]['image_link'])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            img = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        entity_name = self.data.iloc[idx]['entity_name']\n",
    "        \n",
    "        # Check if 'entity_value' column exists in the DataFrame\n",
    "        if 'entity_value' in self.data.columns:\n",
    "            entity_value, entity_unit = self.parse_entity_value(self.data.iloc[idx]['entity_value'])\n",
    "        else:\n",
    "            # If 'entity_value' doesn't exist, use placeholder values\n",
    "            entity_value, entity_unit = 0.0, 'unknown'\n",
    "        \n",
    "        entity_name_tensor = torch.zeros(len(self.entity_names), dtype=torch.float32)\n",
    "        entity_name_tensor[self.entity_name_to_idx[entity_name]] = 1\n",
    "        \n",
    "        entity_unit_tensor = torch.zeros(self.max_units, dtype=torch.float32)\n",
    "        if entity_unit in self.entity_units.get(entity_name, []):\n",
    "            unit_idx = self.entity_units[entity_name].index(entity_unit)\n",
    "            entity_unit_tensor[unit_idx] = 1\n",
    "        \n",
    "        return img, entity_name_tensor, torch.tensor(entity_value, dtype=torch.float32), entity_unit_tensor, entity_name\n",
    "\n",
    "    def parse_entity_value(self, value_str):\n",
    "        value, unit = value_str.rsplit(' ', 1)\n",
    "        if unit == 'fluid':\n",
    "            value, unit = value_str.rsplit(' ', 2)[0], ' '.join(value_str.rsplit(' ', 2)[1:])\n",
    "        return float(value), unit\n",
    "\n",
    "# Custom model\n",
    "class ProductImageModel(nn.Module):\n",
    "    def __init__(self, num_entity_names, max_units):\n",
    "        super(ProductImageModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.fc_entity_name = nn.Linear(num_ftrs, num_entity_names).float()\n",
    "        self.fc_entity_value = nn.Linear(num_ftrs, 1).float()\n",
    "        self.fc_entity_units = nn.Linear(num_ftrs, max_units).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        entity_name_out = self.fc_entity_name(features)\n",
    "        entity_value_out = self.fc_entity_value(features).squeeze(1)\n",
    "        entity_units_out = self.fc_entity_units(features)\n",
    "        return entity_name_out, entity_value_out, entity_units_out\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Preprocess and load data\n",
    "    filtered_data = preprocess_data(TRAIN_CSV_PATH)\n",
    "    train_dataset = ProductImageDataset(filtered_data, TRAIN_IMAGES_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ProductImageModel(len(train_dataset.entity_names), train_dataset.max_units)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion_classification = nn.BCEWithLogitsLoss()\n",
    "    criterion_regression = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "    # Initialize the GradScaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        \n",
    "        for i, (images, entity_names, entity_values, entity_units, entity_name_str) in enumerate(progress_bar):\n",
    "            images = images.to(DEVICE)\n",
    "            entity_names = entity_names.to(DEVICE)\n",
    "            entity_values = entity_values.to(DEVICE)\n",
    "            entity_units = entity_units.to(DEVICE)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                entity_name_out, entity_value_out, entity_units_out = model(images)\n",
    "                \n",
    "                # Compute losses\n",
    "                loss_entity_name = criterion_classification(entity_name_out, entity_names)\n",
    "                loss_entity_value = criterion_regression(entity_value_out, entity_values)\n",
    "                loss_entity_units = criterion_classification(entity_units_out, entity_units)\n",
    "                \n",
    "                loss = loss_entity_name + loss_entity_value + loss_entity_units\n",
    "                loss = loss / ACCUMULATION_STEPS  # Normalize the loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            progress_bar.set_postfix({'loss': loss.item() * ACCUMULATION_STEPS})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save({\n",
    "        'epoch': NUM_EPOCHS,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,  # This line is now safe as avg_loss is defined in the loop\n",
    "    }, MODEL_PATH)\n",
    "\n",
    "    print(\"Training completed. Final model saved.\")\n",
    "\n",
    "# Prediction function\n",
    "@torch.no_grad()\n",
    "def predict(model, image, train_dataset):\n",
    "    model.eval()\n",
    "    with autocast():\n",
    "        entity_name_out, entity_value_out, entity_units_out = model(image.unsqueeze(0).to(DEVICE))\n",
    "        \n",
    "        predicted_entity = train_dataset.entity_names[torch.argmax(entity_name_out).item()]\n",
    "        predicted_value = entity_value_out.item()\n",
    "        predicted_unit_idx = torch.argmax(entity_units_out).item()\n",
    "        predicted_unit = train_dataset.entity_units[predicted_entity][predicted_unit_idx] if predicted_unit_idx < len(train_dataset.entity_units[predicted_entity]) else 'unknown'\n",
    "        \n",
    "        return predicted_entity, predicted_value, predicted_unit\n",
    "\n",
    "# Predictor function for the sample code\n",
    "def predictor(image_link, category_id, entity_name, model, train_dataset, transform):\n",
    "    try:\n",
    "        img = Image.open(image_link).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        predicted_entity, predicted_value, predicted_unit = predict(model, img_tensor, train_dataset)\n",
    "        formatted_prediction = f\"{predicted_value:.2f} {predicted_unit}\"\n",
    "        return formatted_prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_link}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Train the model\n",
    "    # train_model()\n",
    "\n",
    "    # Load the trained model for prediction\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = ProductImageDataset(pd.read_csv(TRAIN_CSV_PATH), TRAIN_IMAGES_DIR, transform=transform)\n",
    "    test_dataset = ProductImageDataset(pd.read_csv(TEST_CSV_PATH), TEST_IMAGES_DIR, transform=transform)\n",
    "    \n",
    "    model = ProductImageModel(len(train_dataset.entity_names), train_dataset.max_units)\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            for image in images:\n",
    "                predicted_entity, predicted_value, predicted_unit = predict(model, image, train_dataset)\n",
    "                formatted_prediction = f\"{predicted_value:.2f} {predicted_unit}\"\n",
    "                predictions.append(formatted_prediction)\n",
    "                print(f\"Predicted: {formatted_prediction}\")  # Print each prediction\n",
    "\n",
    "    # Save predictions\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    test_df['prediction'] = predictions\n",
    "    test_df[['index', 'prediction']].to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"Predictions saved to {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the TrOCR model and processor\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-stage1')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-stage1')\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using TrOCR\n",
    "def extract_text(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=5):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using TrOCR\n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()\n",
    "\n",
    "# Reference: https://huggingface.co/microsoft/trocr-base-stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='ocr_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Tesseract OCR\n",
    "def extract_text(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=5):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        if extracted_text is not None:\n",
    "            print(f\"Image: {row['image_link']}\")\n",
    "            print(f\"Entity Name: {row['entity_name']}\")\n",
    "            print(f\"Actual Label: {row['entity_value']}\")\n",
    "            print(f\"Extracted Text: {extracted_text}\")\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            print(f\"Failed to process image: {row['image_link']}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_random_samples()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        print(\"Tesseract is not installed or not in your PATH.\")\n",
    "        print(\"Please install Tesseract using your package manager.\")\n",
    "        print(\"For Ubuntu/Debian: sudo apt-get install tesseract-ocr\")\n",
    "        print(\"For CentOS/RHEL: sudo yum install tesseract\")\n",
    "        print(\"For more information, visit: https://github.com/tesseract-ocr/tessdoc/blob/main/Installation.md\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        logging.error(f\"Unexpected error in main execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Initialize the EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])  # Adjust languages as needed\n",
    "\n",
    "# Function to extract text from an image using EasyOCR\n",
    "def extract_text(image_path):\n",
    "    result = reader.readtext(image_path)\n",
    "    return ' '.join([text for _, text, _ in result])\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=5):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using EasyOCR\n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Donut\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    # task_prompt = f\"<s_docvqa><s_question>Extract all {entity_name} measurement quantities with units and numerical quantity of the unit, strictly numeircal measurement quantities with units only</s_question><s_answer>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = processor.token2json(sequence)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=15):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using Donut\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\", use_fast=False)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Donut\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Prepare decoder inputs with a suitable prompt for our task\n",
    "    task_prompt = \"<s_rvlcdip>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=15):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using Donut\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = \"\"\"entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'maximum_weight_recommendation': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'}\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from constants import entity_unit_map  # Import entity_unit_map from constants.py\n",
    "\n",
    "# Load the LLaVA model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Add <image> token to tokenizer and resize model embeddings\n",
    "processor.tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Set up multi-GPU processing if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using LLaVA\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the specific units for the given entity_name\n",
    "    entity_units = entity_unit_map.get(entity_name, [])\n",
    "    units_str = \", \".join(entity_units) if entity_units else \"unknown\"\n",
    "    \n",
    "    prompt = f\"\"\"USER: <image>\n",
    "Extract numerical quantities and their corresponding unit belonging to the class `{entity_name}` from the image. \n",
    "\n",
    "Output a JSON in the following format:\n",
    "```\n",
    "{{\n",
    "    [\n",
    "        {{\n",
    "            \"value\": `Double Float`,\n",
    "            \"unit\": `String` (one of: {units_str})\n",
    "        }},\n",
    "        <Repeat for other quantities>\n",
    "    ]\n",
    "}}\n",
    "ASSISTANT:\"\"\"\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():  # Use mixed precision\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            outputs = model.module.generate(**inputs, max_new_tokens=300)\n",
    "        else:\n",
    "            outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=20):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using LLaVA\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        # Extract only the ASSISTANT message from the extracted text\n",
    "        assistant_message = extracted_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {assistant_message}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache before running\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the Donut model and processor\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Donut\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # task_prompt = \"<s_rvlcdip>\"\n",
    "    # task_prompt = \"<s_cord-v2>\"\n",
    "    task_prompt = f\"<s_docvqa><s_question> Extract measurement quantities with units and numerical quantity of the unit corresponding to {entity_name} </s_question><s_answer>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    print(processor.batch_decode(outputs.sequences))\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = processor.token2json(sequence)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=15):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using Donut\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Load the MGP-STR processor and model\n",
    "processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\n",
    "model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\n",
    "\n",
    "# Function to extract text from an image using MGP-STR\n",
    "def extract_text(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(pixel_values)\n",
    "\n",
    "    # Decode the output\n",
    "    result = processor.batch_decode(outputs.logits)\n",
    "    return result['generated_text']\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(train_df, n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(train_df, train_images_dir, num_samples=5):\n",
    "    samples = get_random_samples(train_df, num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(train_images_dir, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using MGP-STR\n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # Load the train.csv file\n",
    "    train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "    # Set the path to your train images directory\n",
    "    TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "    process_random_samples(train_df, TRAIN_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import hjson\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "from sanity import sanity_check\n",
    "from src.utils import download_images\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = True\n",
    "# Try to import bitsandbytes and set up quantization\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    # Set up quantization configuration\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Set up the pipeline with quantization\n",
    "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "except ImportError:\n",
    "    print(\"bitsandbytes is not installed. Falling back to default configuration.\")\n",
    "    \n",
    "    # Set up the pipeline without quantization\n",
    "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    pipe = pipeline(\"image-to-text\", model=model_id)\n",
    "    print(\"Using default configuration without quantization.\")\n",
    "\n",
    "# Load the test.csv file\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the path to your test images directory\n",
    "TEST_IMAGES_DIR = 'test_images'\n",
    "\n",
    "# Download test images if not already downloaded\n",
    "download_images(test_df, TEST_IMAGES_DIR)\n",
    "\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    entity_units = entity_unit_map.get(entity_name, [])\n",
    "    units_str = \", \".join(entity_units) if entity_units else \"unknown\"\n",
    "    \n",
    "    prompt = f\"\"\"USER: <image>\n",
    "Extract numerical quantities and their corresponding unit belonging to the class `{entity_name}` from the image. \n",
    "\n",
    "Output a JSON in the following format:\n",
    "```\n",
    "{{\n",
    "    \"predictions\": [\n",
    "        {{\n",
    "            \"value\": <Double Float>,\n",
    "            \"unit\": <String> (one of: {units_str})\n",
    "        }},\n",
    "        <Repeat for other quantities> (max 3)\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "If no relevant information is found, return an empty list.\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 1024})\n",
    "    return outputs[0]['generated_text']\n",
    "\n",
    "def process_extracted_text(text):\n",
    "    try:\n",
    "        # Extract the JSON part from the response\n",
    "        json_str = text.split(\"```\")[1].strip()\n",
    "        data = hjson.loads(json_str)\n",
    "        predictions = data.get(\"predictions\", [])\n",
    "        \n",
    "        if predictions:\n",
    "            # Perform max voting\n",
    "            value_unit_pairs = [(pred['value'], pred['unit']) for pred in predictions]\n",
    "            if value_unit_pairs:\n",
    "                most_common = max(set(value_unit_pairs), key=value_unit_pairs.count)\n",
    "                return f\"{most_common[0]:.2f} {most_common[1]}\"\n",
    "            else:\n",
    "                # If no pairs found, return the first entry\n",
    "                pred = predictions[0]\n",
    "                return f\"{pred['value']:.2f} {pred['unit']}\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def process_test_set():\n",
    "    predictions = []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        image_path = os.path.join(TEST_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        prediction = process_extracted_text(extracted_text)\n",
    "        predictions.append(prediction)\n",
    "        print(f\"Predicted: {prediction}\")\n",
    "        \n",
    "    test_df['prediction'] = predictions\n",
    "    output_df = test_df[['index', 'prediction']]\n",
    "    output_df.to_csv('test_out.csv', index=False)\n",
    "    print(\"Predictions saved to test_out.csv\")\n",
    "\n",
    "    # Run sanity check\n",
    "    sanity_check('dataset/test.csv', 'test_out.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    process_test_set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "from sanity import sanity_check\n",
    "from src.utils import download_images\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the LLaVA model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "processor.tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, processor):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.img_dir, os.path.basename(row['image_link']))\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        entity_name = row['entity_name']\n",
    "        \n",
    "        entity_units = entity_unit_map.get(entity_name, [])\n",
    "        units_str = \", \".join(entity_units) if entity_units else \"unknown\"\n",
    "        \n",
    "        prompt = f\"\"\"USER: <image>\n",
    "Extract numerical quantities and their corresponding unit belonging to the class `{entity_name}` from the image. \n",
    "\n",
    "Output a JSON in the following format:\n",
    "```\n",
    "{{\n",
    "    \"predictions\": [\n",
    "        {{\n",
    "            \"value\": <Double Float>,\n",
    "            \"unit\": <String> (one of: {units_str})\n",
    "        }},\n",
    "        <Repeat for other quantities> (max 3)\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "If no relevant information is found, return an empty list.\n",
    "ASSISTANT:\"\"\"\n",
    "        \n",
    "        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'pixel_values': inputs.pixel_values.squeeze(),\n",
    "            'input_ids': inputs.input_ids.squeeze(),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(),\n",
    "            'image_path': image_path,\n",
    "            'entity_name': entity_name\n",
    "        }\n",
    "\n",
    "# Batch processing function\n",
    "def process_batch(batch, model, processor):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model.generate(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=4096\n",
    "        )\n",
    "    \n",
    "    generated_texts = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return generated_texts\n",
    "\n",
    "def process_extracted_text(text):\n",
    "    try:\n",
    "        json_start = text.find('ASSISTANT:') + len('ASSISTANT:')\n",
    "        json_text = text[json_start:].strip()\n",
    "        \n",
    "        import hjson\n",
    "        data = hjson.loads(json_text)\n",
    "        predictions = data.get(\"predictions\", [])\n",
    "        \n",
    "        if predictions:\n",
    "            value_unit_pairs = [(pred['value'], pred['unit']) for pred in predictions]\n",
    "            if value_unit_pairs:\n",
    "                most_common = max(set(value_unit_pairs), key=value_unit_pairs.count)\n",
    "                return f\"{most_common[0]:.2f} {most_common[1]}\"\n",
    "            else:\n",
    "                pred = predictions[0]\n",
    "                return f\"{pred['value']:.2f} {pred['unit']}\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except hjson.HjsonDecodeError:\n",
    "        print(f\"Failed to parse Hjson: {json_text}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_test_set():\n",
    "    # Load the test.csv file\n",
    "    test_df = pd.read_csv('dataset/test.csv')\n",
    "    \n",
    "    # Set the path to your test images directory\n",
    "    TEST_IMAGES_DIR = 'test_images'\n",
    "    \n",
    "    # Download test images if not already downloaded\n",
    "    download_images(test_df, TEST_IMAGES_DIR)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = ImageDataset(test_df, TEST_IMAGES_DIR, processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        generated_texts = process_batch(batch, model, processor)\n",
    "        \n",
    "        for text in generated_texts:\n",
    "            prediction = process_extracted_text(text)\n",
    "            predictions.append(prediction)\n",
    "            print(f\"Predicted: {prediction}\")\n",
    "    \n",
    "    test_df['prediction'] = predictions\n",
    "    output_df = test_df[['index', 'prediction']]\n",
    "    output_df.to_csv('test_out.csv', index=False)\n",
    "    print(\"Predictions saved to test_out.csv\")\n",
    "\n",
    "    # Run sanity check\n",
    "    sanity_check('dataset/test.csv', 'test_out.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    process_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize OCR model\n",
    "model_ocr = ocr_predictor(pretrained=True, assume_straight_pages=False, export_as_straight_boxes=True)\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the path to your train/test images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "TEST_IMAGES_DIR = 'test_images'\n",
    "\n",
    "# Function to extract text from the OCR result\n",
    "def extract_text(ocr_result):\n",
    "    extracted_text = []\n",
    "    for page in ocr_result['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for line in block['lines']:\n",
    "                for word in line['words']:\n",
    "                    extracted_text.append(word['value'])\n",
    "    return ' '.join(extracted_text)\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        result = model_ocr(doc)\n",
    "        extracted_text = extract_text(result.export())\n",
    "        return extracted_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Filter the text for numerical values and context\n",
    "def filter_text(text):\n",
    "    sentences = re.findall(r'([^.]*?\\d+[^.]*\\.)', text)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "# Preprocess data for training\n",
    "def preprocess_data(df, images_dir):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing data\"):\n",
    "        image_path = os.path.join(images_dir, os.path.basename(row['image_link']))\n",
    "        extracted_text = process_image(image_path)\n",
    "        filtered_text = filter_text(extracted_text)\n",
    "        entity_value = row['entity_value']\n",
    "        \n",
    "        if str(entity_value) in filtered_text:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        \n",
    "        texts.append(filtered_text)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Prepare the dataset\n",
    "print(\"Preparing training dataset...\")\n",
    "texts, labels = preprocess_data(train_df, TRAIN_IMAGES_DIR)\n",
    "\n",
    "# Tokenize the text data\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "# Split into training and evaluation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Set up the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Prediction function using BERT\n",
    "def predict_entity_value(entity_name, extracted_text):\n",
    "    filtered_text = filter_text(extracted_text)\n",
    "    inputs = tokenizer(filtered_text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    if prediction == 1:\n",
    "        entity_units = entity_unit_map.get(entity_name, set())\n",
    "        unit_pattern = '|'.join(entity_units)\n",
    "        candidates = re.findall(rf'\\d+(\\.\\d+)?\\s*({unit_pattern})', filtered_text)\n",
    "        if candidates:\n",
    "            return f\"{candidates[0][0]} {candidates[0][1]}\"\n",
    "    return \"\"\n",
    "\n",
    "# Perform predictions on test set\n",
    "print(\"Performing predictions on test set...\")\n",
    "predictions = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting\"):\n",
    "    image_path = os.path.join(TEST_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "    extracted_text = process_image(image_path)\n",
    "    entity_name = row['entity_name']\n",
    "    \n",
    "    predicted_value = predict_entity_value(entity_name, extracted_text)\n",
    "    predictions.append(predicted_value)\n",
    "\n",
    "# Create output DataFrame\n",
    "output_df = pd.DataFrame({\n",
    "    'index': test_df.index,\n",
    "    'prediction': predictions\n",
    "})\n",
    "output_df.to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"Prediction process completed. Results saved to 'test_out.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "import random\n",
    "\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize OCR model\n",
    "model_ocr = ocr_predictor(pretrained=True, assume_straight_pages=False, export_as_straight_boxes=True)\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from the OCR result\n",
    "def extract_text(ocr_result):\n",
    "    extracted_text = []\n",
    "    for page in ocr_result['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for line in block['lines']:\n",
    "                for word in line['words']:\n",
    "                    extracted_text.append(word['value'])\n",
    "    return ' '.join(extracted_text)\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        result = model_ocr(doc)\n",
    "        extracted_text = extract_text(result.export())\n",
    "        return extracted_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Filter the text for numerical values and context\n",
    "def filter_text(text):\n",
    "    sentences = re.findall(r'([^.]*?\\d+[^.]*\\.)', text)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9.\\s]', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Prediction function using BERT\n",
    "def predict_entity_value(entity_name, extracted_text):\n",
    "    filtered_text = filter_text(extracted_text)\n",
    "    preprocessed_text = preprocess_text(filtered_text)\n",
    "    inputs = tokenizer(preprocessed_text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    if prediction == 1:\n",
    "        entity_units = entity_unit_map.get(entity_name, set())\n",
    "        unit_pattern = '|'.join(entity_units)\n",
    "        candidates = re.findall(rf'\\d+(\\.\\d+)?\\s*({unit_pattern})', preprocessed_text)\n",
    "        if candidates:\n",
    "            return f\"{candidates[0][0]} {candidates[0][1]}\"\n",
    "    return \"\"\n",
    "\n",
    "# Perform predictions on 5-10 random samples\n",
    "num_samples = random.randint(5, 10)\n",
    "sample_df = train_df.sample(n=num_samples)\n",
    "\n",
    "print(f\"Performing predictions on {num_samples} random samples...\")\n",
    "for _, row in tqdm(sample_df.iterrows(), total=num_samples, desc=\"Predicting\"):\n",
    "    image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "    extracted_text = process_image(image_path)\n",
    "    entity_name = row['entity_name']\n",
    "    actual_value = row['entity_value']\n",
    "    \n",
    "    predicted_value = predict_entity_value(entity_name, extracted_text)\n",
    "    \n",
    "    print(f\"Input: Entity: {entity_name}, Extracted Text: {extracted_text}\")\n",
    "    print(f\"Labelled Output: {actual_value}\")\n",
    "    print(f\"Prediction: {predicted_value}\")\n",
    "    print(\"-\" * 100)  # Separator for readability\n",
    "\n",
    "print(\"Inference on random samples completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/doctr/models/utils/pytorch.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(archive_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing predictions on 15 random samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   7%|         | 1/15 [00:00<00:09,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: width, Extracted Text: Size ofPumpkins - N - - 6.5cm 8cm  \n",
      "Labelled Output: 8.0 centimetre\n",
      "Prediction: 6.5 centimetre\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  13%|        | 2/15 [00:01<00:12,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: wattage, Extracted Text: S FRINTING MOONLIGHT L un Haepas  NOON NIG Gc GIE tvn me  e 2AS EaN b / 1V a 0008 LEDWILB (0 5c 6 L & 5.9in G6 - - 3D User 16Colors Manual Moon Lamp 6  R d -\n",
      "Labelled Output: 1.0 watt\n",
      "Prediction:  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  20%|        | 3/15 [00:02<00:10,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: width, Extracted Text: Enlarged Letter Tray Organize More Files 13.7\" 4 TV 3.3\" 9.2\"\n",
      "Labelled Output: 9.2 inch\n",
      "Prediction: 13.7 inch\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  27%|       | 4/15 [00:04<00:14,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: item_weight, Extracted Text: ase auy CLASSIC NESCAFE a CI NE pa - NESC consumarsi A,7g aconn - CLASSIC NESCAFE rM CLASSIC WVEVTO entro pe  ou - Mptpees ouM AA . entroline NESCAFO CLASSIC NESCAFE CLASSIC NESCAFE  1,7g,DOECON CLASSIC 2 CLASSIC ESCAFE NESCAFE NESCAFE. BUSIW a086w9 MnP 810 NESCAFE, SASASAER 6NSAGOLRZO entrotine NNS 4ASS/C - 6W5 NESCAFE. MooI MEG ERUNATAZA 024 > - WASSAYSNN WAYATANNDNC Cumi 100% omnd NODNISER OH oXOnOoweRwATOkO Pavvolyent Gustointenso PURO CAFEE OMEVISQONEKOARZON FIIEMSS a CAFFE OIEZHSOOOLESORZON AOAASMOmA au a  - I &y A -  a\n",
      "Labelled Output: 1.7 gram\n",
      "Prediction: 7.0 gram\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  33%|      | 5/15 [00:05<00:11,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: item_weight, Extracted Text: NATURE'S BOUNTY NOIIn1055I0 RAPIDE BIOTINE 2500 mcg  Aide  maintenir la sant des cheveux et de la peau Aide  soutenir la sant des ongles 100 Comprims NPN 80043208\n",
      "Labelled Output: 2500.0 microgram\n",
      "Prediction:  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  40%|      | 6/15 [00:06<00:09,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: item_weight, Extracted Text: TGLCO. THE GOOD LIFE COMPANY MORNING MOTIVATION FREEZE-DRIED FROM 100% ARABICA BEANS 100g/35\n",
      "Labelled Output: 100 gram\n",
      "Prediction: 100.0 gram\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  47%|     | 7/15 [00:07<00:07,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: width, Extracted Text: 0.87'caliber The 0.87'caliber is suitable for most kinds of bicycles. Universal, fine polished and non-slip, adjustable buckle. -\n",
      "Labelled Output: 0.87 inch\n",
      "Prediction: 0.87 foot\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  53%|    | 8/15 [00:09<00:09,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: item_weight, Extracted Text: VITAMIN D3 Supports Calcium / Metabolism, Bone PACK Health and Immune Function* Jarrow Formulase Vitamin D3 provides cholecalcifero: Jarrow which is the form produced FORMULAS by the (human) skin in Golecddifero response to UVB exposure Vitamin In Extra Virgin Olive Oil (sunlight) and may also 125 MCG improve vitamin D status NI000S dE better than equivalent Supports Caldum Metabolism* amounts of ergocalcifero Bone Health* puD Immune Function* (D2).* 100 SUPPLEMENT DIETARY SOFTGELS *These statements have not been evaluated by the Food and Drug Administration. This product is not intended to diagnose treat, cure or preventany disease.\n",
      "Labelled Output: 125.0 microgram\n",
      "Prediction:  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  60%|    | 9/15 [00:09<00:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: item_weight, Extracted Text: SOURGE llon FIBS Sugartree / - gh Gleie Choco Chip Biscuits 125 04402\n",
      "Labelled Output: 125.0 gram\n",
      "Prediction:  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  67%|   | 10/15 [00:10<00:04,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: width, Extracted Text: 145cm 40cm\n",
      "Labelled Output: 40.0 centimetre\n",
      "Prediction: 145.0 centimetre\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  73%|  | 11/15 [00:11<00:03,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: depth, Extracted Text: Wall Mirror & Home decor Mirror Oval imylmes G 24\" Horizontal or vertical hanging 36\" TII\n",
      "Labelled Output: 36.0 inch\n",
      "Prediction: 36.0 inch\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  80%|  | 12/15 [00:13<00:04,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: wattage, Extracted Text: Dimensions and Accessories 10cm/3.94in 20cm/7.87in 8cm/3.15in Aufladbare Innen Wandleuchte Betriebsanleitung (Deutsch) Bei diesem Produkt handelt tessichum eine Wandleuchte mit Bewegungssensor. Bitte uberprufen Sie. ob Teile beschadigt sind. wenn Sie die Leuchte erhalten. + W/Y Das Produkt kann mit einem wiederauadbaren Zyklus betrieben werdenund wird mit einem us-Schontstelenkabel geliefert Es kann zum Aufladen direkt an das Produkt angeschlossen werden, die : - Ladestatusanzeige ist Die Ladestatusanzeige istrot und blinkt wahrend NET: des Ladevorgangs sie hort auf zu blinken, wenn das Gerat vollstandig geladen ist. 3H Warum leuchtet sie tagsUber nicht? Produkt ist lichtempfindich und IduAIOAsCAkoNwC 5H bei guten Mrccurcalnd ausgeschalte. und der LL dunklen Lichtverhalinissen automatisch 8H Spezifikationen: MNasw  Lichtquelle: Wiederaufladbar Batterie * LED-Licht a Lichtfarbe: Licht SEALANTFIX warmes Montage: Klebrig. magnetisch - Erfassungsbereich 0-3m Erfassungswinkel: 120 Betriebsspannung: 5V Leistung: 3W B\n",
      "Labelled Output: 3.0 watt\n",
      "Prediction: 3.0 watt\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  87%| | 13/15 [00:14<00:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: item_weight, Extracted Text: NEW! - GUSTO PAPRIKA CHRS EXTRA GVSTOSE - - No) ol1o 14 S SEMI I4 GIRASOLE SENZA GWTINE PALM OIL 25g a * FREE\n",
      "Labelled Output: 25.0 gram\n",
      "Prediction: 4.0 gram\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  93%|| 14/15 [00:16<00:01,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: width, Extracted Text: Androic 7.1.2 - OS Built-in Android OS.download APP,online movies,play games 06:12 de ) d a a Goog You Tube NETFLIX Google play A ls -  - China Dofar o FileExplorer Sanvry a o - a - Gr SAbg Screen Apps HDMI - Settings Settings aouu a A OIE OTHA MNPROJECTOR Compatible with multiple devices - ( % - - 1SAS WiFi Bluetooth 2.4G/5G\n",
      "Labelled Output: 3.0 inch\n",
      "Prediction:  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|| 15/15 [00:17<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Entity: height, Extracted Text: Product Size 5.5\"/14cm 15.7\"/40cm 7.9\"/20cm 0000 C - s 000000 U 0000C - Bottles Weight Dimensions Outlet 6-8 5.51 Ib 15.7'Lx7.9\"H 100-240V 40cm X 20cm X 14cm\n",
      "Labelled Output: 14.0 centimetre\n",
      "Prediction: 14.0 centimetre\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Inference on random samples completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "import random\n",
    "\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize OCR model\n",
    "model_ocr = ocr_predictor(pretrained=True, assume_straight_pages=False, export_as_straight_boxes=True)\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from the OCR result\n",
    "def extract_text(ocr_result):\n",
    "    extracted_text = []\n",
    "    for page in ocr_result['pages']:\n",
    "        for block in page['blocks']:\n",
    "            for line in block['lines']:\n",
    "                for word in line['words']:\n",
    "                    extracted_text.append(word['value'])\n",
    "    return ' '.join(extracted_text)\n",
    "\n",
    "# Function to process a single image\n",
    "def process_image(image_path):\n",
    "    try:\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        result = model_ocr(doc)\n",
    "        extracted_text = extract_text(result.export())\n",
    "        return extracted_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9.\\s\\'\"]', ' ', text)  # Remove special characters except ' and \"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Unit standardization\n",
    "unit_mappings = {\n",
    "    # Length units\n",
    "    'cm': 'centimetre', 'mm': 'millimetre', 'm': 'metre', 'in': 'inch', 'ft': 'foot', 'yd': 'yard',\n",
    "    'centimetres': 'centimetre', 'millimetres': 'millimetre', 'metres': 'metre',\n",
    "    'inches': 'inch', 'feet': 'foot', 'yards': 'yard',\n",
    "    'cm.': 'centimetre', 'mm.': 'millimetre', 'm.': 'metre', 'in.': 'inch', 'ft.': 'foot', 'yd.': 'yard',\n",
    "    '\"': 'inch', \"'\": 'foot', 'candelas': 'candela', 'cd': 'candela', 'candela': 'candela', 'candelas': 'candela', 'candel': 'candela',\n",
    "\n",
    "    # Weight units\n",
    "    'mg': 'milligram', 'g': 'gram', 'kg': 'kilogram', 'oz': 'ounce', 'lb': 'pound', 't': 'ton',\n",
    "    'g': 'microgram', 'microg': 'microgram',\n",
    "    'grams': 'gram', 'kilograms': 'kilogram', 'milligrams': 'milligram', 'ounces': 'ounce',\n",
    "    'pounds': 'pound', 'tons': 'ton', 'lbs': 'pound',\n",
    "    'mg.': 'milligram', 'g.': 'gram', 'kg.': 'kilogram', 'oz.': 'ounce', 'lb.': 'pound', 'lbs.': 'pound',\n",
    "    'g.': 'microgram',\n",
    "\n",
    "    # Volume units\n",
    "    'ml': 'millilitre', 'l': 'litre', 'cl': 'centilitre', 'dl': 'decilitre', 'fl oz': 'fluid ounce',\n",
    "    'pt': 'pint', 'qt': 'quart', 'gal': 'gallon', 'imp gal': 'imperial gallon',\n",
    "    'cu ft': 'cubic foot', 'cu in': 'cubic inch', 'cu m': 'cubic metre',\n",
    "    'l': 'microlitre', 'microl': 'microlitre', 'c': 'cup',\n",
    "    'litres': 'litre', 'millilitres': 'millilitre', 'centilitres': 'centilitre', 'decilitres': 'decilitre',\n",
    "    'pints': 'pint', 'quarts': 'quart', 'gallons': 'gallon', 'imperial gallons': 'imperial gallon',\n",
    "    'cubic feet': 'cubic foot', 'cubic inches': 'cubic inch', 'cubic meters': 'cubic metre',\n",
    "    'cubic metres': 'cubic metre', 'fluid ounces': 'fluid ounce', 'cups': 'cup',\n",
    "    'ml.': 'millilitre', 'l.': 'litre', 'cl.': 'centilitre', 'dl.': 'decilitre',\n",
    "    'pt.': 'pint', 'qt.': 'quart', 'gal.': 'gallon',\n",
    "    'cu. ft.': 'cubic foot', 'cu. in.': 'cubic inch', 'cu. m': 'cubic metre',\n",
    "    'fl. oz.': 'fluid ounce', 'fluid oz': 'fluid ounce', 'fluid oz.': 'fluid ounce',\n",
    "    'us gal': 'gallon', 'us gallon': 'gallon', 'us gallons': 'gallon',\n",
    "    'us fl oz': 'fluid ounce', 'us fluid ounce': 'fluid ounce', 'us fluid ounces': 'fluid ounce',\n",
    "    'us pint': 'pint', 'us pints': 'pint', 'us quart': 'quart', 'us quarts': 'quart',\n",
    "    'microlitres': 'microlitre',\n",
    "\n",
    "    # Electrical units\n",
    "    'mv': 'millivolt', 'kv': 'kilovolt', 'v': 'volt',\n",
    "    'w': 'watt', 'kw': 'kilowatt',\n",
    "    'volts': 'volt', 'millivolts': 'millivolt', 'kilovolts': 'kilovolt',\n",
    "    'watts': 'watt', 'kilowatts': 'kilowatt',\n",
    "    'mv.': 'millivolt', 'kv.': 'kilovolt', 'v.': 'volt',\n",
    "    'w.': 'watt', 'kw.': 'kilowatt'\n",
    "}\n",
    "\n",
    "def standardize_unit(unit):\n",
    "    unit = unit.lower()\n",
    "    return unit_mappings.get(unit, unit)\n",
    "\n",
    "# Entity-specific patterns\n",
    "entity_patterns = {\n",
    "    'item_weight': r'(\\d+(\\.\\d+)?)\\s*(milligram|kilogram|microgram|gram|ounce|ton|pound|mg|kg|g|oz|lb|lbs|g|t|microg|grams|kilograms|milligrams|ounces|pounds|tons)',\n",
    "    'maximum_weight_recommendation': r'(\\d+(\\.\\d+)?)\\s*(milligram|kilogram|microgram|gram|ounce|ton|pound|mg|kg|g|oz|lb|lbs|g|t|microg|grams|kilograms|milligrams|ounces|pounds|tons)',\n",
    "    'width': r'(\\d+(\\.\\d+)?)\\s*(centimetre|foot|millimetre|metre|inch|yard|cm|mm|m|in|ft|yd|centimetres|millimetres|metres|inches|feet|yards|\"|\\')',\n",
    "    'height': r'(\\d+(\\.\\d+)?)\\s*(centimetre|foot|millimetre|metre|inch|yard|cm|mm|m|in|ft|yd|centimetres|millimetres|metres|inches|feet|yards|\"|\\')',\n",
    "    'depth': r'(\\d+(\\.\\d+)?)\\s*(centimetre|foot|millimetre|metre|inch|yard|cm|mm|m|in|ft|yd|centimetres|millimetres|metres|inches|feet|yards|\"|\\')',\n",
    "    'voltage': r'(\\d+(\\.\\d+)?)\\s*(millivolt|kilovolt|volt|mv|kv|v|volts|millivolts|kilovolts)',\n",
    "    'wattage': r'(\\d+(\\.\\d+)?)\\s*(kilowatt|watt|kw|w|watts|kilowatts)',\n",
    "    'item_volume': r'(\\d+(\\.\\d+)?)\\s*(cubic foot|cubic metre|microlitre|cup|fluid ounce|centilitre|imperial gallon|us gallon|pint|decilitre|litre|millilitre|quart|cubic inch|gallon|cu ft|cu m|cu in|ml|l|cl|dl|fl oz|pt|qt|gal|imp gal|us gal|l|c|microl|cubic feet|cubic inches|cubic meters|cubic metres|fluid ounces|gallons|imperial gallons|us gallons|litres|millilitres|pints|quarts|us pints|us quarts|us fluid ounces)'\n",
    "}\n",
    "\n",
    "# Contextual keyword matching\n",
    "entity_keywords = {\n",
    "    'item_weight': ['weight', 'weighs', 'mass', 'heavy', 'light', 'lb', 'kg', 'grams', 'ounces', 'net weight', 'gross weight', 'tare weight', 'product weight', 'item weight', 'unit weight', 'shipping weight', 'payload', 'heft', 'load', 'bulk', 'density', 'avoirdupois', 'poundage', 'tonnage', 'weightiness', 'gravitas', 'ponderosity', 'substance', 'ballast', 'burden', 'encumbrance', 'gravity', 'heaviness', 'mass', 'pressure', 'tonnage', 'weight force', 'weightage', 'dead weight', 'live weight', 'curb weight', 'dry weight', 'unladen weight', 'laden weight', 'kerb weight', 'gross vehicle weight', 'candle'],\n",
    "    'maximum_weight_recommendation': ['max weight', 'maximum weight', 'weight limit', 'weight capacity', 'load capacity', 'can hold up to', 'supports up to', 'max load', 'weight rating', 'safe working load', 'recommended max weight', 'weight restriction', 'not to exceed', 'maximum load', 'weight threshold', 'upper weight limit', 'peak weight', 'weight tolerance', 'maximum carrying capacity', 'weight bearing limit', 'load limit', 'weight allowance', 'maximum permissible weight', 'weight ceiling', 'weight boundary', 'weight cutoff', 'weight maximum', 'weight cap', 'weight constraint', 'weight barrier', 'weight ceiling', 'weight threshold', 'weight upper bound', 'weight top end', 'weight peak', 'weight apex', 'weight zenith', 'weight summit', 'weight pinnacle', 'weight acme'],\n",
    "    'width': ['width', 'wide', 'across', 'breadth', 'span', 'horizontal', 'side to side', 'lateral', 'diameter', 'girth', 'W:', 'W.', 'width:', 'wide:', 'cross section', 'transverse dimension', 'broadness', 'wideness', 'beam', 'thickness', 'gauge', 'caliber', 'amplitude', 'expanse', 'spread', 'broadness', 'extent', 'measurement', 'size', 'dimension', 'proportion', 'magnitude', 'scope', 'range', 'compass', 'reach', 'extension', 'expansion', 'stretch', 'span', 'latitude', 'bore', 'calibre', 'thickness', 'cross-section', 'profile'],\n",
    "    'height': ['height', 'tall', 'high', 'elevation', 'vertical length', 'stature', 'altitude', 'top to bottom', 'upright', 'rise', 'H:', 'H.', 'height:', 'tall:', 'vertical dimension', 'clearance', 'tallness', 'highness', 'loftiness', 'prominence', 'eminence', 'towering', 'vertical extent', 'headroom', 'ceiling height', 'vertical distance', 'vertical measurement', 'vertical span', 'vertical reach', 'vertical dimension', 'vertical size', 'vertical proportion', 'vertical magnitude', 'vertical scope', 'vertical range', 'vertical compass', 'vertical extension', 'vertical expansion', 'vertical stretch', 'vertical elevation', 'vertical rise', 'vertical lift', 'vertical climb', 'vertical ascent', 'vertical growth'],\n",
    "    'depth': ['depth', 'deep', 'thickness', 'front to back', 'length', 'extent', 'distance', 'profundity', 'dimension', 'reach', 'D:', 'D.', 'depth:', 'deep:', 'longitudinal dimension', 'deepness', 'profoundness', 'penetration', 'recession', 'inwardness', 'immersion', 'submersion', 'sinking', 'hollowness', 'concavity', 'vertical distance', 'vertical extent', 'vertical dimension', 'vertical measurement', 'vertical reach', 'vertical penetration', 'vertical recession', 'vertical immersion', 'vertical submersion', 'vertical sinking', 'vertical depression', 'vertical cavity', 'vertical hollow', 'vertical recess', 'vertical indentation', 'vertical pit', 'vertical chasm', 'vertical abyss', 'vertical gorge', 'vertical ravine'],\n",
    "    'voltage': ['voltage', 'volts', 'V', 'electrical potential', 'electromotive force', 'power supply', 'input voltage', 'output voltage', 'operating voltage', 'rated voltage', 'AC voltage', 'DC voltage', 'potential difference', 'electric pressure', 'tension', 'EMF', 'volt rating', 'voltage drop', 'voltage range', 'nominal voltage', 'supply voltage', 'line voltage', 'phase voltage', 'peak voltage', 'RMS voltage', 'breakdown voltage', 'threshold voltage', 'forward voltage', 'reverse voltage', 'standoff voltage', 'surge voltage', 'ripple voltage', 'voltage regulation', 'voltage stability', 'voltage tolerance', 'voltage fluctuation', 'voltage sag', 'voltage spike', 'voltage dip', 'voltage surge', 'voltage transient'],\n",
    "    'wattage': ['wattage', 'watts', 'power', 'energy consumption', 'power output', 'W', 'power rating', 'power consumption', 'energy usage', 'power draw', 'electrical power', 'rated power', 'power capacity', 'energy demand', 'power requirement', 'power level', 'energy efficiency', 'power dissipation', 'power supply', 'power specification', 'power demand', 'power input', 'power output', 'power throughput', 'power handling', 'power delivery', 'power transfer', 'power conversion', 'power generation', 'power production', 'power yield', 'power expenditure', 'power utilization', 'power allocation', 'power budget', 'power threshold', 'power limit', 'power range', 'power margin', 'power reserve'],\n",
    "    'item_volume': ['volume', 'capacity', 'contains', 'content', 'holds', 'storage', 'liquid capacity', 'fluid volume', 'internal volume', 'container volume', 'total volume', 'net volume', 'gross volume', 'fill capacity', 'cubic capacity', 'volumetric capacity', 'displacement', 'interior space', 'holding capacity', 'storage space', 'cubic volume', 'spatial volume', 'volumetric content', 'volumetric measurement', 'volumetric size', 'volumetric dimension', 'volumetric extent', 'volumetric magnitude', 'volumetric quantity', 'volumetric amount', 'volumetric proportion', 'volumetric ratio', 'volumetric fraction', 'volumetric part', 'volumetric segment', 'volumetric section', 'volumetric division', 'volumetric portion', 'volumetric share', 'volumetric allotment']\n",
    "}\n",
    "\n",
    "def extract_value_unit(text, pattern, allowed_units):\n",
    "    if not isinstance(pattern, str):\n",
    "        return []\n",
    "    matches = re.findall(pattern, text)\n",
    "    extractions = []\n",
    "    for match in matches:\n",
    "        value = float(match[0])\n",
    "        unit = match[2]\n",
    "        unit_standard = standardize_unit(unit)\n",
    "        if unit_standard in allowed_units:\n",
    "            extractions.append((value, unit_standard))\n",
    "    return extractions\n",
    "\n",
    "def find_value_with_context(text, entity, pattern, allowed_units):\n",
    "    if not isinstance(pattern, str):\n",
    "        return []\n",
    "    keywords = entity_keywords.get(entity, [])\n",
    "    extractions = []\n",
    "    for keyword in keywords:\n",
    "        keyword_positions = [m.start() for m in re.finditer(keyword, text)]\n",
    "        for pos in keyword_positions:\n",
    "            window = text[max(0, pos - 50): pos + 50]\n",
    "            extractions.extend(extract_value_unit(window, pattern, allowed_units))\n",
    "    if not extractions:\n",
    "        extractions = extract_value_unit(text, pattern, allowed_units)\n",
    "    return extractions\n",
    "\n",
    "def extract_entity_value(entity, text):\n",
    "    clean_text = preprocess_text(text)\n",
    "    allowed_units = entity_unit_map.get(entity, set())\n",
    "    pattern = entity_patterns.get(entity, '')\n",
    "    extractions = find_value_with_context(clean_text, entity, pattern, allowed_units)\n",
    "    \n",
    "    if extractions:\n",
    "        if entity in ['width', 'height', 'depth']:\n",
    "            # Check if we have multiple extractions close together (potential dimensions)\n",
    "            if len(extractions) >= 2:\n",
    "                # Sort extractions based on their position in the text\n",
    "                sorted_extractions = sorted(extractions, key=lambda x: clean_text.find(f\"{x[0]} {x[1]}\"))\n",
    "                \n",
    "                if entity == 'width':\n",
    "                    return sorted_extractions[0]\n",
    "                elif entity == 'depth':\n",
    "                    return sorted_extractions[1] if len(sorted_extractions) > 1 else sorted_extractions[0]\n",
    "                elif entity == 'height':\n",
    "                    return sorted_extractions[-1]  # Return the last value for height\n",
    "            else:\n",
    "                # If we only have one extraction, return it\n",
    "                return extractions[0]\n",
    "        else:\n",
    "            return extractions[0]\n",
    "    \n",
    "    return \"\", \"\"\n",
    "\n",
    "# Perform predictions on 5-10 random samples\n",
    "num_samples = 15\n",
    "sample_df = train_df.sample(n=num_samples)\n",
    "\n",
    "print(f\"Performing predictions on {num_samples} random samples...\")\n",
    "for _, row in tqdm(sample_df.iterrows(), total=num_samples, desc=\"Predicting\"):\n",
    "    image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "    extracted_text = process_image(image_path)\n",
    "    entity_name = row['entity_name']\n",
    "    actual_value = row['entity_value']\n",
    "    \n",
    "    predicted_value, predicted_unit = extract_entity_value(entity_name, extracted_text)\n",
    "    \n",
    "    print(f\"Input: Entity: {entity_name}, Extracted Text: {extracted_text}\")\n",
    "    print(f\"Labelled Output: {actual_value}\")\n",
    "    print(f\"Prediction: {predicted_value} {predicted_unit}\")\n",
    "    print(\"-\" * 100)  # Separator for readability\n",
    "\n",
    "print(\"Inference on random samples completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting:  33%|      | 5/15 [00:05<00:11,  1.11s/it]\n",
    "Input: Entity: item_volume, Extracted Text: E 7 CAFFEINE N A C * BETA-ALANINE J - / ALINE 7 CAFFEINE * I 7 CAFFEINE L CAFFEINE OSE * BETA-ALANINE A CA - 7 CAFFEINE LE BETA-ALANINE a BETA-ALANINE # AFFEINE a BETA-ALANIN a BETA-ALANINE N 4 1 - I - 0 - ON - n  YUZU - - - ECHELON CAYENNE ECHE 12 PACK 12 x 8.4 FL OZ YUZU CAYENNE 7 PRE-ORKOUTINACAN [250mL] cans MADEINTHE DIETARY SUPPLEMENT 8.4 FL OZ (250 mL)\n",
    "Labelled Output: 8.4 fluid ounce\n",
    "Prediction: 7.0 cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '' (U+2588) (3575373954.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Predicting:  60%|    | 9/15 [00:08<00:05,  1.07it/s]\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '' (U+2588)\n"
     ]
    }
   ],
   "source": [
    "Predicting:  60%|    | 9/15 [00:08<00:05,  1.07it/s]\n",
    "Input: Entity: item_weight, Extracted Text:   - ASIANAURA Scented Pillar Candie Set of 4 Candle ASIAN/AURA ASIAN/AURA Lemon Grass ASIANAURA Scented Lemon Grass Pillar Candle Scented Lemon Grass ASIANAURA Pillar Candle jented Lemon Pillar Candle Grass Scented Pillar Cand h\n",
    "Labelled Output: 4 candela\n",
    "Prediction:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicting:  40%|      | 6/15 [00:05<00:09,  1.01s/it]\n",
    "Input: Entity: width, Extracted Text: FOOD GRADE 304 STAINLESS STEEL You can take it with you when you travel. The compact design saves you a lot of space when packing. Chenpi Pu'er Tea Sweet honey water Warm lemon tea Health wolfberry water 205mm/8.07inch Espresso Baby's milk >  205mm/8.07nch\n",
    "Labelled Output: 205.0 millimetre\n",
    "Prediction: 8.07 inch\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
