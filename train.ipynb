{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from src.constants import entity_unit_map\n",
    "import ast\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom dataset\n",
    "class ProductImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.entity_names = sorted(self.data['entity_name'].unique())\n",
    "        self.entity_name_to_idx = {name: idx for idx, name in enumerate(self.entity_names)}\n",
    "        self.entity_units = {entity: sorted(units) for entity, units in entity_unit_map.items()}\n",
    "        self.max_units = max(len(units) for units in self.entity_units.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.basename(self.data.iloc[idx]['image_link'])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        entity_name = self.data.iloc[idx]['entity_name']\n",
    "        entity_value, entity_unit = self.parse_entity_value(self.data.iloc[idx]['entity_value'])\n",
    "        \n",
    "        entity_name_tensor = torch.zeros(len(self.entity_names), dtype=torch.float32)\n",
    "        entity_name_tensor[self.entity_name_to_idx[entity_name]] = 1\n",
    "        \n",
    "        entity_unit_tensor = torch.zeros(self.max_units, dtype=torch.float32)\n",
    "        if entity_name in self.entity_units and entity_unit in self.entity_units[entity_name]:\n",
    "            unit_idx = self.entity_units[entity_name].index(entity_unit)\n",
    "            entity_unit_tensor[unit_idx] = 1\n",
    "        \n",
    "        return img, entity_name_tensor, torch.tensor(entity_value, dtype=torch.float32), entity_unit_tensor, entity_name\n",
    "\n",
    "    def parse_entity_value(self, value_str):\n",
    "        try:\n",
    "            value, unit = value_str.rsplit(' ', 1)\n",
    "            if unit == 'fluid':\n",
    "                value, unit = value_str.rsplit(' ', 2)[0], ' '.join(value_str.rsplit(' ', 2)[1:])\n",
    "            return float(value), unit\n",
    "        except ValueError:\n",
    "            try:\n",
    "                value_list = ast.literal_eval(value_str)\n",
    "                if isinstance(value_list, list) and len(value_list) == 2:\n",
    "                    return sum(value_list) / 2, 'unknown'\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return 0.0, 'unknown'\n",
    "\n",
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_dataset = ProductImageDataset('dataset/train.csv', TRAIN_IMAGES_DIR, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Custom model\n",
    "class ProductImageModel(nn.Module):\n",
    "    def __init__(self, num_entity_names, max_units):\n",
    "        super(ProductImageModel, self).__init__()\n",
    "        self.resnet = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.fc_entity_name = nn.Linear(num_ftrs, num_entity_names).float()\n",
    "        self.fc_entity_value = nn.Linear(num_ftrs, 1).float()\n",
    "        self.fc_entity_units = nn.Linear(num_ftrs, max_units).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        entity_name_out = self.fc_entity_name(features)\n",
    "        entity_value_out = self.fc_entity_value(features).squeeze(1)\n",
    "        entity_units_out = self.fc_entity_units(features)\n",
    "        return entity_name_out, entity_value_out, entity_units_out\n",
    "\n",
    "# Initialize model\n",
    "model = ProductImageModel(len(train_dataset.entity_names), train_dataset.max_units)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_classification = nn.BCEWithLogitsLoss()\n",
    "criterion_regression = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    for images, entity_names, entity_values, entity_units, entity_name_str in progress_bar:\n",
    "        images = images.to(DEVICE).float()\n",
    "        entity_names = entity_names.to(DEVICE).float()\n",
    "        entity_values = entity_values.to(DEVICE).float()\n",
    "        entity_units = entity_units.to(DEVICE).float()\n",
    "        \n",
    "        # Forward pass\n",
    "        entity_name_out, entity_value_out, entity_units_out = model(images)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss_entity_name = criterion_classification(entity_name_out, entity_names)\n",
    "        loss_entity_value = criterion_regression(entity_value_out, entity_values)\n",
    "        loss_entity_units = criterion_classification(entity_units_out, entity_units)\n",
    "        \n",
    "        loss = loss_entity_name + loss_entity_value + loss_entity_units\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "# Save the model\n",
    "torch.save({\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_loss,\n",
    "}, 'product_image_model.pth')\n",
    "\n",
    "print(\"Training completed. Model saved.\")\n",
    "\n",
    "# Evaluation and prediction function\n",
    "def predict(model, image):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        entity_name_out, entity_value_out, entity_units_out = model(image.unsqueeze(0).to(DEVICE).float())\n",
    "        \n",
    "        predicted_entity = train_dataset.entity_names[torch.argmax(entity_name_out).item()]\n",
    "        predicted_value = entity_value_out.item()\n",
    "        predicted_unit_idx = torch.argmax(entity_units_out).item()\n",
    "        predicted_unit = train_dataset.entity_units.get(predicted_entity, ['unknown'])[predicted_unit_idx] if predicted_unit_idx < len(train_dataset.entity_units.get(predicted_entity, [])) else 'unknown'\n",
    "        \n",
    "        return predicted_entity, predicted_value, predicted_unit\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, _, _, _, entity_name_str in tqdm(train_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(DEVICE).float()\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            predicted_entity, predicted_value, predicted_unit = predict(model, image)\n",
    "            formatted_prediction = f\"{predicted_value:.2f} {predicted_unit}\"\n",
    "            predictions.append(formatted_prediction)\n",
    "            \n",
    "            if predicted_entity == entity_name_str[i]:\n",
    "                correct_predictions += 1\n",
    "            total_samples += 1\n",
    "\n",
    "accuracy = correct_predictions / total_samples\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save predictions\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "test_df['prediction'] = predictions[:len(test_df)]  # Ensure we have the correct number of predictions\n",
    "test_df[['index', 'prediction']].to_csv('test_out.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to test_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import ast\n",
    "\n",
    "# Allow loading truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Define constants\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "TEST_IMAGES_DIR = 'test_images'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ACCUMULATION_STEPS = 2\n",
    "CHECKPOINT_PATH = 'product_image_model_checkpoint.pth'\n",
    "MODEL_PATH = 'product_image_model_final.pth'\n",
    "TRAIN_CSV_PATH = 'dataset/train.csv'\n",
    "TEST_CSV_PATH = 'dataset/test.csv'\n",
    "OUTPUT_CSV_PATH = 'dataset/test_out.csv'\n",
    "\n",
    "# Load entity_unit_map\n",
    "with open('src/constants.py', 'r') as f:\n",
    "    exec(f.read())\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    original_count = len(data)\n",
    "    \n",
    "    def is_valid_sample(row):\n",
    "        try:\n",
    "            value_str = row['entity_value']\n",
    "            entity_name = row['entity_name']\n",
    "            value, unit = value_str.rsplit(' ', 1)\n",
    "            if unit == 'fluid':\n",
    "                value, unit = value_str.rsplit(' ', 2)[0], ' '.join(value_str.rsplit(' ', 2)[1:])\n",
    "            float(value)  # Check if value can be converted to float\n",
    "            return unit in entity_unit_map.get(entity_name, [])\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    filtered_data = data[data.apply(is_valid_sample, axis=1)]\n",
    "    filtered_count = len(filtered_data)\n",
    "    \n",
    "    print(f\"Original sample count: {original_count}\")\n",
    "    print(f\"Filtered sample count: {filtered_count}\")\n",
    "    print(f\"Removed {original_count - filtered_count} samples ({(original_count - filtered_count) / original_count:.2%})\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "# Custom dataset\n",
    "class ProductImageDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, transform=None):\n",
    "        self.data = data\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.entity_names = sorted(self.data['entity_name'].unique())\n",
    "        self.entity_name_to_idx = {name: idx for idx, name in enumerate(self.entity_names)}\n",
    "        self.entity_units = {entity: sorted(units) for entity, units in entity_unit_map.items()}\n",
    "        self.max_units = max(len(units) for units in self.entity_units.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.basename(self.data.iloc[idx]['image_link'])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "            img = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        entity_name = self.data.iloc[idx]['entity_name']\n",
    "        \n",
    "        # Check if 'entity_value' column exists in the DataFrame\n",
    "        if 'entity_value' in self.data.columns:\n",
    "            entity_value, entity_unit = self.parse_entity_value(self.data.iloc[idx]['entity_value'])\n",
    "        else:\n",
    "            # If 'entity_value' doesn't exist, use placeholder values\n",
    "            entity_value, entity_unit = 0.0, 'unknown'\n",
    "        \n",
    "        entity_name_tensor = torch.zeros(len(self.entity_names), dtype=torch.float32)\n",
    "        entity_name_tensor[self.entity_name_to_idx[entity_name]] = 1\n",
    "        \n",
    "        entity_unit_tensor = torch.zeros(self.max_units, dtype=torch.float32)\n",
    "        if entity_unit in self.entity_units.get(entity_name, []):\n",
    "            unit_idx = self.entity_units[entity_name].index(entity_unit)\n",
    "            entity_unit_tensor[unit_idx] = 1\n",
    "        \n",
    "        return img, entity_name_tensor, torch.tensor(entity_value, dtype=torch.float32), entity_unit_tensor, entity_name\n",
    "\n",
    "    def parse_entity_value(self, value_str):\n",
    "        value, unit = value_str.rsplit(' ', 1)\n",
    "        if unit == 'fluid':\n",
    "            value, unit = value_str.rsplit(' ', 2)[0], ' '.join(value_str.rsplit(' ', 2)[1:])\n",
    "        return float(value), unit\n",
    "\n",
    "# Custom model\n",
    "class ProductImageModel(nn.Module):\n",
    "    def __init__(self, num_entity_names, max_units):\n",
    "        super(ProductImageModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        self.fc_entity_name = nn.Linear(num_ftrs, num_entity_names).float()\n",
    "        self.fc_entity_value = nn.Linear(num_ftrs, 1).float()\n",
    "        self.fc_entity_units = nn.Linear(num_ftrs, max_units).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        entity_name_out = self.fc_entity_name(features)\n",
    "        entity_value_out = self.fc_entity_value(features).squeeze(1)\n",
    "        entity_units_out = self.fc_entity_units(features)\n",
    "        return entity_name_out, entity_value_out, entity_units_out\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Preprocess and load data\n",
    "    filtered_data = preprocess_data(TRAIN_CSV_PATH)\n",
    "    train_dataset = ProductImageDataset(filtered_data, TRAIN_IMAGES_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ProductImageModel(len(train_dataset.entity_names), train_dataset.max_units)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion_classification = nn.BCEWithLogitsLoss()\n",
    "    criterion_regression = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "    # Initialize the GradScaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Load checkpoint if it exists\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        \n",
    "        for i, (images, entity_names, entity_values, entity_units, entity_name_str) in enumerate(progress_bar):\n",
    "            images = images.to(DEVICE)\n",
    "            entity_names = entity_names.to(DEVICE)\n",
    "            entity_values = entity_values.to(DEVICE)\n",
    "            entity_units = entity_units.to(DEVICE)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast():\n",
    "                # Forward pass\n",
    "                entity_name_out, entity_value_out, entity_units_out = model(images)\n",
    "                \n",
    "                # Compute losses\n",
    "                loss_entity_name = criterion_classification(entity_name_out, entity_names)\n",
    "                loss_entity_value = criterion_regression(entity_value_out, entity_values)\n",
    "                loss_entity_units = criterion_classification(entity_units_out, entity_units)\n",
    "                \n",
    "                loss = loss_entity_name + loss_entity_value + loss_entity_units\n",
    "                loss = loss / ACCUMULATION_STEPS  # Normalize the loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i + 1) % ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            progress_bar.set_postfix({'loss': loss.item() * ACCUMULATION_STEPS})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, CHECKPOINT_PATH)\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Save the final model\n",
    "    torch.save({\n",
    "        'epoch': NUM_EPOCHS,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': avg_loss,  # This line is now safe as avg_loss is defined in the loop\n",
    "    }, MODEL_PATH)\n",
    "\n",
    "    print(\"Training completed. Final model saved.\")\n",
    "\n",
    "# Prediction function\n",
    "@torch.no_grad()\n",
    "def predict(model, image, train_dataset):\n",
    "    model.eval()\n",
    "    with autocast():\n",
    "        entity_name_out, entity_value_out, entity_units_out = model(image.unsqueeze(0).to(DEVICE))\n",
    "        \n",
    "        predicted_entity = train_dataset.entity_names[torch.argmax(entity_name_out).item()]\n",
    "        predicted_value = entity_value_out.item()\n",
    "        predicted_unit_idx = torch.argmax(entity_units_out).item()\n",
    "        predicted_unit = train_dataset.entity_units[predicted_entity][predicted_unit_idx] if predicted_unit_idx < len(train_dataset.entity_units[predicted_entity]) else 'unknown'\n",
    "        \n",
    "        return predicted_entity, predicted_value, predicted_unit\n",
    "\n",
    "# Predictor function for the sample code\n",
    "def predictor(image_link, category_id, entity_name, model, train_dataset, transform):\n",
    "    try:\n",
    "        img = Image.open(image_link).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        predicted_entity, predicted_value, predicted_unit = predict(model, img_tensor, train_dataset)\n",
    "        formatted_prediction = f\"{predicted_value:.2f} {predicted_unit}\"\n",
    "        return formatted_prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_link}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Train the model\n",
    "    # train_model()\n",
    "\n",
    "    # Load the trained model for prediction\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = ProductImageDataset(pd.read_csv(TRAIN_CSV_PATH), TRAIN_IMAGES_DIR, transform=transform)\n",
    "    test_dataset = ProductImageDataset(pd.read_csv(TEST_CSV_PATH), TEST_IMAGES_DIR, transform=transform)\n",
    "    \n",
    "    model = ProductImageModel(len(train_dataset.entity_names), train_dataset.max_units)\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, _, _, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            images = images.to(DEVICE)\n",
    "            \n",
    "            for image in images:\n",
    "                predicted_entity, predicted_value, predicted_unit = predict(model, image, train_dataset)\n",
    "                formatted_prediction = f\"{predicted_value:.2f} {predicted_unit}\"\n",
    "                predictions.append(formatted_prediction)\n",
    "                print(f\"Predicted: {formatted_prediction}\")  # Print each prediction\n",
    "\n",
    "    # Save predictions\n",
    "    test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "    test_df['prediction'] = predictions\n",
    "    test_df[['index', 'prediction']].to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"Predictions saved to {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the TrOCR model and processor\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-stage1')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-stage1')\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using TrOCR\n",
    "def extract_text(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=5):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using TrOCR\n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()\n",
    "\n",
    "# Reference: https://huggingface.co/microsoft/trocr-base-stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='ocr_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Tesseract OCR\n",
    "def extract_text(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing image {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=5):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        if extracted_text is not None:\n",
    "            print(f\"Image: {row['image_link']}\")\n",
    "            print(f\"Entity Name: {row['entity_name']}\")\n",
    "            print(f\"Actual Label: {row['entity_value']}\")\n",
    "            print(f\"Extracted Text: {extracted_text}\")\n",
    "            print(\"-\" * 50)\n",
    "        else:\n",
    "            print(f\"Failed to process image: {row['image_link']}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_random_samples()\n",
    "    except pytesseract.TesseractNotFoundError:\n",
    "        print(\"Tesseract is not installed or not in your PATH.\")\n",
    "        print(\"Please install Tesseract using your package manager.\")\n",
    "        print(\"For Ubuntu/Debian: sudo apt-get install tesseract-ocr\")\n",
    "        print(\"For CentOS/RHEL: sudo yum install tesseract\")\n",
    "        print(\"For more information, visit: https://github.com/tesseract-ocr/tessdoc/blob/main/Installation.md\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        logging.error(f\"Unexpected error in main execution: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Initialize the EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])  # Adjust languages as needed\n",
    "\n",
    "# Function to extract text from an image using EasyOCR\n",
    "def extract_text(image_path):\n",
    "    result = reader.readtext(image_path)\n",
    "    return ' '.join([text for _, text, _ in result])\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=5):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using EasyOCR\n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"jinhybr/OCR-Donut-CORD\")\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Donut\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    # task_prompt = f\"<s_docvqa><s_question>Extract all {entity_name} measurement quantities with units and numerical quantity of the unit, strictly numeircal measurement quantities with units only</s_question><s_answer>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        pixel_values,\n",
    "        decoder_input_ids=decoder_input_ids,\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=1,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = processor.token2json(sequence)\n",
    "\n",
    "    return sequence\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=15):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using Donut\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the Donut model and processor\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\", use_fast=False)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Donut\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Prepare decoder inputs with a suitable prompt for our task\n",
    "    task_prompt = \"<s_rvlcdip>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=15):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using Donut\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = \"\"\"entity_unit_map = {\n",
    "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
    "    'item_weight': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'maximum_weight_recommendation': {'gram',\n",
    "        'kilogram',\n",
    "        'microgram',\n",
    "        'milligram',\n",
    "        'ounce',\n",
    "        'pound',\n",
    "        'ton'},\n",
    "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
    "    'wattage': {'kilowatt', 'watt'},\n",
    "    'item_volume': {'centilitre',\n",
    "        'cubic foot',\n",
    "        'cubic inch',\n",
    "        'cup',\n",
    "        'decilitre',\n",
    "        'fluid ounce',\n",
    "        'gallon',\n",
    "        'imperial gallon',\n",
    "        'litre',\n",
    "        'microlitre',\n",
    "        'millilitre',\n",
    "        'pint',\n",
    "        'quart'}\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from constants import entity_unit_map  # Import entity_unit_map from constants.py\n",
    "\n",
    "# Load the LLaVA model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "# Add <image> token to tokenizer and resize model embeddings\n",
    "processor.tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Set up multi-GPU processing if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using LLaVA\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the specific units for the given entity_name\n",
    "    entity_units = entity_unit_map.get(entity_name, [])\n",
    "    units_str = \", \".join(entity_units) if entity_units else \"unknown\"\n",
    "    \n",
    "    prompt = f\"\"\"USER: <image>\n",
    "Extract numerical quantities and their corresponding unit belonging to the class `{entity_name}` from the image. \n",
    "\n",
    "Output a JSON in the following format:\n",
    "```\n",
    "{{\n",
    "    [\n",
    "        {{\n",
    "            \"value\": `Double Float`,\n",
    "            \"unit\": `String` (one of: {units_str})\n",
    "        }},\n",
    "        <Repeat for other quantities>\n",
    "    ]\n",
    "}}\n",
    "ASSISTANT:\"\"\"\n",
    "    \n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():  # Use mixed precision\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            outputs = model.module.generate(**inputs, max_new_tokens=300)\n",
    "        else:\n",
    "            outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "    generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=20):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using LLaVA\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        # Extract only the ASSISTANT message from the extracted text\n",
    "        assistant_message = extracted_text.split(\"ASSISTANT:\")[-1].strip()\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {assistant_message}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()  # Clear CUDA cache before running\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Load the Donut model and processor\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n",
    "# processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load the train.csv file\n",
    "train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "# Set the path to your train images directory\n",
    "TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "# Function to extract text from an image using Donut\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # task_prompt = \"<s_rvlcdip>\"\n",
    "    # task_prompt = \"<s_cord-v2>\"\n",
    "    task_prompt = f\"<s_docvqa><s_question> Extract measurement quantities with units and numerical quantity of the unit corresponding to {entity_name} </s_question><s_answer>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    print(processor.batch_decode(outputs.sequences))\n",
    "    sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    sequence = processor.token2json(sequence)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(num_samples=15):\n",
    "    samples = get_random_samples(num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(TRAIN_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using Donut\n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    process_random_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MgpstrProcessor, MgpstrForSceneTextRecognition\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# Load the MGP-STR processor and model\n",
    "processor = MgpstrProcessor.from_pretrained('alibaba-damo/mgp-str-base')\n",
    "model = MgpstrForSceneTextRecognition.from_pretrained('alibaba-damo/mgp-str-base')\n",
    "\n",
    "# Function to extract text from an image using MGP-STR\n",
    "def extract_text(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Perform inference\n",
    "    outputs = model(pixel_values)\n",
    "\n",
    "    # Decode the output\n",
    "    result = processor.batch_decode(outputs.logits)\n",
    "    return result['generated_text']\n",
    "\n",
    "# Function to get random samples from the train set\n",
    "def get_random_samples(train_df, n=5):\n",
    "    return train_df.sample(n)\n",
    "\n",
    "# Main function to process random samples\n",
    "def process_random_samples(train_df, train_images_dir, num_samples=5):\n",
    "    samples = get_random_samples(train_df, num_samples)\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        image_path = os.path.join(train_images_dir, os.path.basename(row['image_link']))\n",
    "        \n",
    "        # Extract text using MGP-STR\n",
    "        extracted_text = extract_text(image_path)\n",
    "        \n",
    "        print(f\"Image: {row['image_link']}\")\n",
    "        print(f\"Entity Name: {row['entity_name']}\")\n",
    "        print(f\"Actual Label: {row['entity_value']}\")\n",
    "        print(f\"Extracted Text: {extracted_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # Load the train.csv file\n",
    "    train_df = pd.read_csv('dataset/train.csv')\n",
    "\n",
    "    # Set the path to your train images directory\n",
    "    TRAIN_IMAGES_DIR = 'train_images'\n",
    "\n",
    "    process_random_samples(train_df, TRAIN_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import hjson\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "from sanity import sanity_check\n",
    "from src.utils import download_images\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = True\n",
    "# Try to import bitsandbytes and set up quantization\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    # Set up quantization configuration\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Set up the pipeline with quantization\n",
    "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})\n",
    "    print(\"Using 4-bit quantization.\")\n",
    "except ImportError:\n",
    "    print(\"bitsandbytes is not installed. Falling back to default configuration.\")\n",
    "    \n",
    "    # Set up the pipeline without quantization\n",
    "    model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    pipe = pipeline(\"image-to-text\", model=model_id)\n",
    "    print(\"Using default configuration without quantization.\")\n",
    "\n",
    "# Load the test.csv file\n",
    "test_df = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "# Set the path to your test images directory\n",
    "TEST_IMAGES_DIR = 'test_images'\n",
    "\n",
    "# Download test images if not already downloaded\n",
    "download_images(test_df, TEST_IMAGES_DIR)\n",
    "\n",
    "def extract_text(image_path, entity_name):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    entity_units = entity_unit_map.get(entity_name, [])\n",
    "    units_str = \", \".join(entity_units) if entity_units else \"unknown\"\n",
    "    \n",
    "    prompt = f\"\"\"USER: <image>\n",
    "Extract numerical quantities and their corresponding unit belonging to the class `{entity_name}` from the image. \n",
    "\n",
    "Output a JSON in the following format:\n",
    "```\n",
    "{{\n",
    "    \"predictions\": [\n",
    "        {{\n",
    "            \"value\": <Double Float>,\n",
    "            \"unit\": <String> (one of: {units_str})\n",
    "        }},\n",
    "        <Repeat for other quantities> (max 3)\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "If no relevant information is found, return an empty list.\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 1024})\n",
    "    return outputs[0]['generated_text']\n",
    "\n",
    "def process_extracted_text(text):\n",
    "    try:\n",
    "        # Extract the JSON part from the response\n",
    "        json_str = text.split(\"```\")[1].strip()\n",
    "        data = hjson.loads(json_str)\n",
    "        predictions = data.get(\"predictions\", [])\n",
    "        \n",
    "        if predictions:\n",
    "            # Perform max voting\n",
    "            value_unit_pairs = [(pred['value'], pred['unit']) for pred in predictions]\n",
    "            if value_unit_pairs:\n",
    "                most_common = max(set(value_unit_pairs), key=value_unit_pairs.count)\n",
    "                return f\"{most_common[0]:.2f} {most_common[1]}\"\n",
    "            else:\n",
    "                # If no pairs found, return the first entry\n",
    "                pred = predictions[0]\n",
    "                return f\"{pred['value']:.2f} {pred['unit']}\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def process_test_set():\n",
    "    predictions = []\n",
    "    \n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        image_path = os.path.join(TEST_IMAGES_DIR, os.path.basename(row['image_link']))\n",
    "        \n",
    "        extracted_text = extract_text(image_path, row['entity_name'])\n",
    "        prediction = process_extracted_text(extracted_text)\n",
    "        predictions.append(prediction)\n",
    "        print(f\"Predicted: {prediction}\")\n",
    "        \n",
    "    test_df['prediction'] = predictions\n",
    "    output_df = test_df[['index', 'prediction']]\n",
    "    output_df.to_csv('test_out.csv', index=False)\n",
    "    print(\"Predictions saved to test_out.csv\")\n",
    "\n",
    "    # Run sanity check\n",
    "    sanity_check('dataset/test.csv', 'test_out.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    process_test_set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 3/3 [00:02<00:00,  1.40it/s]\n",
      "  0%|          | 4/131187 [00:09<82:12:53,  2.26s/it] \n",
      "  0%|          | 0/32797 [00:00<?, ?it/s]/tmp/ipykernel_32382/1951208854.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "  0%|          | 1/32797 [00:08<81:54:09,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 2000.00 metre\n",
      "Predicted: 5.56 centimetre\n",
      "Predicted: 1.50 yard\n",
      "Predicted: 5.56 centimetre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/32797 [00:16<75:13:44,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 13.13 metre\n",
      "Predicted: 13.13 metre\n",
      "Predicted: 13.13 metre\n",
      "Predicted: 1250.00 foot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 317, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in <dictcomp>\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 214, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [133] at entry 0 and [137] at entry 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 147\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    146\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mprocess_test_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 129\u001b[0m, in \u001b[0;36mprocess_test_set\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    127\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 129\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerated_texts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerated_texts\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1324\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1323\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/ml/lib/python3.11/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 317, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 155, in <dictcomp>\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/.conda/envs/ml/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 214, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [133] at entry 0 and [137] at entry 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from constants import entity_unit_map\n",
    "from sanity import sanity_check\n",
    "from src.utils import download_images\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'true'\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the LLaVA model and processor\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "processor.tokenizer.add_tokens([\"<image>\"], special_tokens=True)\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "# Custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, processor):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.img_dir, os.path.basename(row['image_link']))\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        entity_name = row['entity_name']\n",
    "        \n",
    "        entity_units = entity_unit_map.get(entity_name, [])\n",
    "        units_str = \", \".join(entity_units) if entity_units else \"unknown\"\n",
    "        \n",
    "        prompt = f\"\"\"USER: <image>\n",
    "Extract numerical quantities and their corresponding unit belonging to the class `{entity_name}` from the image. \n",
    "\n",
    "Output a JSON in the following format:\n",
    "```\n",
    "{{\n",
    "    \"predictions\": [\n",
    "        {{\n",
    "            \"value\": <Double Float>,\n",
    "            \"unit\": <String> (one of: {units_str})\n",
    "        }},\n",
    "        <Repeat for other quantities> (max 3)\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "If no relevant information is found, return an empty list.\n",
    "ASSISTANT:\"\"\"\n",
    "        \n",
    "        inputs = self.processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "        return {\n",
    "            'pixel_values': inputs.pixel_values.squeeze(),\n",
    "            'input_ids': inputs.input_ids.squeeze(),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(),\n",
    "            'image_path': image_path,\n",
    "            'entity_name': entity_name\n",
    "        }\n",
    "\n",
    "# Batch processing function\n",
    "def process_batch(batch, model, processor):\n",
    "    pixel_values = batch['pixel_values'].to(device)\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "        outputs = model.generate(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=4096\n",
    "        )\n",
    "    \n",
    "    generated_texts = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return generated_texts\n",
    "\n",
    "def process_extracted_text(text):\n",
    "    try:\n",
    "        json_start = text.find('ASSISTANT:') + len('ASSISTANT:')\n",
    "        json_text = text[json_start:].strip()\n",
    "        \n",
    "        import hjson\n",
    "        data = hjson.loads(json_text)\n",
    "        predictions = data.get(\"predictions\", [])\n",
    "        \n",
    "        if predictions:\n",
    "            value_unit_pairs = [(pred['value'], pred['unit']) for pred in predictions]\n",
    "            if value_unit_pairs:\n",
    "                most_common = max(set(value_unit_pairs), key=value_unit_pairs.count)\n",
    "                return f\"{most_common[0]:.2f} {most_common[1]}\"\n",
    "            else:\n",
    "                pred = predictions[0]\n",
    "                return f\"{pred['value']:.2f} {pred['unit']}\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    except hjson.HjsonDecodeError:\n",
    "        print(f\"Failed to parse Hjson: {json_text}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_test_set():\n",
    "    # Load the test.csv file\n",
    "    test_df = pd.read_csv('dataset/test.csv')\n",
    "    \n",
    "    # Set the path to your test images directory\n",
    "    TEST_IMAGES_DIR = 'test_images'\n",
    "    \n",
    "    # Download test images if not already downloaded\n",
    "    download_images(test_df, TEST_IMAGES_DIR)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = ImageDataset(test_df, TEST_IMAGES_DIR, processor)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        generated_texts = process_batch(batch, model, processor)\n",
    "        \n",
    "        for text in generated_texts:\n",
    "            prediction = process_extracted_text(text)\n",
    "            predictions.append(prediction)\n",
    "            print(f\"Predicted: {prediction}\")\n",
    "    \n",
    "    test_df['prediction'] = predictions\n",
    "    output_df = test_df[['index', 'prediction']]\n",
    "    output_df.to_csv('test_out.csv', index=False)\n",
    "    print(\"Predictions saved to test_out.csv\")\n",
    "\n",
    "    # Run sanity check\n",
    "    sanity_check('dataset/test.csv', 'test_out.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    process_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
