{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItaW8_9StOW5",
        "outputId": "761d590d-8378-4539-acd7-5b334b4d2e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.17.0\n",
            "Uninstalling tensorflow-2.17.0:\n",
            "  Successfully uninstalled tensorflow-2.17.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting python-doctr[torch]\n",
            "  Downloading python_doctr-0.9.0-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.26.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.13.1)\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (3.11.0)\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (4.10.0.84)\n",
            "Collecting pypdfium2<5.0.0,>=4.11.0 (from python-doctr[torch])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m820.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyclipper<2.0.0,>=1.2.0 (from python-doctr[torch])\n",
            "  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: shapely<3.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (2.0.6)\n",
            "Collecting langdetect<2.0.0,>=1.0.9 (from python-doctr[torch])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from python-doctr[torch])\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.24.6)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (9.4.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.7.1)\n",
            "Collecting anyascii>=0.3.2 (from python-doctr[torch])\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch<3.0.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.19.0+cu121)\n",
            "Collecting onnx<3.0.0,>=1.12.0 (from python-doctr[torch])\n",
            "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect<2.0.0,>=1.0.9->python-doctr[torch]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<3.0.0,>=1.12.0->python-doctr[torch]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=1.12.0->python-doctr[torch]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=1.12.0->python-doctr[torch]) (1.3.0)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_doctr-0.9.0-py3-none-any.whl (299 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.5/299.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=0a7ee8e74465823b342a44d329a753f326a99bd64d5208a4e4d01ea9c1e796bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pyclipper, rapidfuzz, pypdfium2, onnx, langdetect, anyascii, python-doctr\n",
            "Successfully installed anyascii-0.3.2 langdetect-1.0.9 onnx-1.16.2 pyclipper-1.3.0.post5 pypdfium2-4.30.0 python-doctr-0.9.0 rapidfuzz-3.9.7\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow  # Remove TensorFlow\n",
        "!pip install torch  # Install PyTorch\n",
        "!pip install python-doctr[torch]  # Install docTR with PyTorch backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd4717qctPds",
        "outputId": "f8826020-72fb-4198-baf0-80a22d5661b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-doctr[torch] in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.26.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.13.1)\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (3.11.0)\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (4.10.0.84)\n",
            "Requirement already satisfied: pypdfium2<5.0.0,>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (4.30.0)\n",
            "Requirement already satisfied: pyclipper<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.3.0.post5)\n",
            "Requirement already satisfied: shapely<3.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (2.0.6)\n",
            "Requirement already satisfied: langdetect<2.0.0,>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (3.9.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.24.6)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (9.4.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.7.1)\n",
            "Requirement already satisfied: anyascii>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (4.66.5)\n",
            "Requirement already satisfied: torch<3.0.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (0.19.0+cu121)\n",
            "Requirement already satisfied: onnx<3.0.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from python-doctr[torch]) (1.16.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect<2.0.0,>=1.0.9->python-doctr[torch]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<3.0.0,>=1.12.0->python-doctr[torch]) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=1.12.0->python-doctr[torch]) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=1.12.0->python-doctr[torch]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0.0,>=0.20.0->python-doctr[torch]) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=1.12.0->python-doctr[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install python-doctr[torch]\n",
        "%pip install fuzzywuzzy\n",
        "%pip install requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9vbPPk3t7Lf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()\n",
        "\n",
        "# Assuming the CSV has a column 'file_path' with image paths\n",
        "file_paths = df['image_link'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LNh82qwTbJS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdxMGhvtOWNR",
        "outputId": "3808547d-206b-4047-9914-621a91f00fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OCR process completed. Results saved to 'generated_text.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from doctr.io import DocumentFile\n",
        "import numpy\n",
        "from doctr.models import ocr_predictor\n",
        "\n",
        "# Initialize OCR model with vertical text handling\n",
        "model = ocr_predictor(pretrained=True, assume_straight_pages=False, export_as_straight_boxes=True)\n",
        "\n",
        "# Create a directory for downloaded images\n",
        "image_dir = 'downloaded_image'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# Load the CSV with URLs\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# List to hold results\n",
        "results = []\n",
        "\n",
        "# Function to extract text from the OCR result\n",
        "def extract_text(ocr_result):\n",
        "    extracted_text = []\n",
        "    # Traverse the result and extract the 'value' fields (words)\n",
        "    for page in ocr_result['pages']:\n",
        "        for block in page['blocks']:\n",
        "            for line in block['lines']:\n",
        "                for word in line['words']:\n",
        "                    extracted_text.append(word['value'])\n",
        "    # Join all words with a space\n",
        "    return ' '.join(extracted_text)\n",
        "\n",
        "# Iterate over each URL in the CSV\n",
        "for idx, row in df.iterrows():\n",
        "    image_url = row['image_link']  # Adjust column name if necessary\n",
        "\n",
        "    try:\n",
        "        # Download the image\n",
        "        response = requests.get(image_url, stream=True)\n",
        "        if response.status_code == 200:\n",
        "            # Create the local file path\n",
        "            image_filename = os.path.join(image_dir, f\"image_{idx}.jpg\")\n",
        "            with open(image_filename, 'wb') as img_file:\n",
        "                img_file.write(response.content)\n",
        "\n",
        "            # Perform OCR on the downloaded image\n",
        "            doc = DocumentFile.from_images(image_filename)\n",
        "            result = model(doc)\n",
        "            extracted_text = extract_text(result.export())  # Extract only the text values\n",
        "            results.append({'file_path': image_url, 'extracted_text': extracted_text})\n",
        "        else:\n",
        "            print(f\"Failed to download {image_url}, status code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_url}: {e}\")\n",
        "\n",
        "# Save the results to a CSV\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv('generated_text.csv', index=False)\n",
        "\n",
        "print(\"OCR process completed. Results saved to 'generated_text.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "entity_unit_map = {\n",
        "    'width': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'depth': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'height': {'centimetre', 'foot', 'inch', 'metre', 'millimetre', 'yard'},\n",
        "    'item_weight': {'gram',\n",
        "        'kilogram',\n",
        "        'microgram',\n",
        "        'milligram',\n",
        "        'ounce',\n",
        "        'pound',\n",
        "        'ton'},\n",
        "    'maximum_weight_recommendation': {'gram',\n",
        "        'kilogram',\n",
        "        'microgram',\n",
        "        'milligram',\n",
        "        'ounce',\n",
        "        'pound',\n",
        "        'ton'},\n",
        "    'voltage': {'kilovolt', 'millivolt', 'volt'},\n",
        "    'wattage': {'kilowatt', 'watt'},\n",
        "    'item_volume': {'centilitre',\n",
        "        'cubic foot',\n",
        "        'cubic inch',\n",
        "        'cup',\n",
        "        'decilitre',\n",
        "        'fluid ounce',\n",
        "        'gallon',\n",
        "        'imperial gallon',\n",
        "        'litre',\n",
        "        'microlitre',\n",
        "        'millilitre',\n",
        "        'pint',\n",
        "        'quart'}\n",
        "}\n",
        "\n",
        "allowed_units = {unit for entity in entity_unit_map for unit in entity_unit_map[entity]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install textblob\n",
        "!pip install scikit-learn\n",
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "generate_df = pd.read_csv('generated_text.csv')\n",
        "\n",
        "# Merge train_df with generate_df on the image_link column to access extracted_text\n",
        "merged_df = pd.merge(train_df, generate_df, left_on='image_link', right_on='file_path', how='inner')\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Filter the text for numerical values and context\n",
        "def filter_text(text):\n",
        "    # Extract only sentences containing numerical values\n",
        "    sentences = re.findall(r'([^.]*?\\d+[^.]*\\.)', text)\n",
        "    return ' '.join(sentences)\n",
        "\n",
        "# Preprocess data for training\n",
        "def preprocess_data(df):\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        extracted_text = filter_text(row['extracted_text'])\n",
        "        entity_value = row['entity_value']\n",
        "        \n",
        "        # Label the text as 1 if it contains the entity value, otherwise 0\n",
        "        if str(entity_value) in extracted_text:\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(0)\n",
        "        \n",
        "        texts.append(extracted_text)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# Prepare the dataset\n",
        "texts, labels = preprocess_data(merged_df)\n",
        "\n",
        "# Tokenize the text data\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Create a PyTorch dataset\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "dataset = CustomDataset(inputs, labels)\n",
        "\n",
        "# Split into training and evaluation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Set up the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Prediction function using BERT\n",
        "def predict_entity_value(entity_name, extracted_text):\n",
        "    filtered_text = filter_text(extracted_text)\n",
        "    inputs = tokenizer(filtered_text, return_tensors='pt', truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "    \n",
        "    # Return prediction if it's positive\n",
        "    if prediction == 1:\n",
        "        # Extract numerical value from filtered text\n",
        "        candidates = re.findall(r'\\d+(\\.\\d+)?\\s*(gram|kilogram|milligram|ounce|pound|litre|cup|gallon|watt)', filtered_text)\n",
        "        if candidates:\n",
        "            return f\"{candidates[0][0]} {candidates[0][1]}\"\n",
        "    return \"\"\n",
        "\n",
        "# Perform predictions on `generate_text.csv`\n",
        "predictions = []\n",
        "for idx, row in generate_df.iterrows():\n",
        "    file_path = row['file_path']\n",
        "    extracted_text = row['extracted_text']\n",
        "    \n",
        "    # Find the entity name for this entry using the merged DataFrame\n",
        "    train_row = merged_df[merged_df['file_path'] == file_path].iloc[0]\n",
        "    entity_name = train_row['entity_name']\n",
        "    actual_value = train_row['entity_value']\n",
        "    \n",
        "    # Predict the value\n",
        "    predicted_value = predict_entity_value(entity_name, extracted_text)\n",
        "    \n",
        "    # Print debugging information\n",
        "    print(f\"Entity: {entity_name}, Entity Value: {actual_value}, Prediction: {predicted_value}, Extracted Text: {extracted_text}\")\n",
        "    \n",
        "    predictions.append(predicted_value)\n",
        "\n",
        "# Evaluate the predictions\n",
        "output_df = pd.DataFrame({\n",
        "    'index': generate_df.index,\n",
        "    'prediction': predictions\n",
        "})\n",
        "output_df.to_csv('output_predictions.csv', index=False)\n",
        "\n",
        "print(\"Prediction process completed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
